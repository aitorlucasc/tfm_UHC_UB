{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"results_mnist_pq_coincidence.ipynb","provenance":[],"collapsed_sections":["Veyl-ielnc8k"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"goVqH-dieDrM"},"source":["!wget https://www.dropbox.com/s/o0e0p2ahkj60bzq/teacher0to6.pt\n","!wget https://www.dropbox.com/s/kze5cdv7dvas3e4/teacher3to9.pt\n","!wget https://www.dropbox.com/s/h7l845py2d3t6o5/data.zip\n","!wget https://www.dropbox.com/s/la55owlh2tnsg0d/u_methods.csv\n","!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XmqRVp61lCFu"},"source":["# LOADING MODEL AND MNIST CLASSES"]},{"cell_type":"code","metadata":{"id":"jOk72lj4eXJD"},"source":["import os\n","import time\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Import torchvision functions/classes for MNIST import and data loaders\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","class Model(nn.Module):\n","\n","    def __init__(self, n_classes, hidden_size=1200, dropout=0.0, hidden_dropout=0.0):\n","        super(Model, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.hidden1 = nn.Linear(784, hidden_size, bias=True)\n","        self.hidden1_dropout = nn.Dropout(hidden_dropout)\n","        self.hidden2 = nn.Linear(hidden_size, hidden_size, bias=True)\n","        self.hidden2_dropout = nn.Dropout(hidden_dropout)\n","        self.hidden3 = nn.Linear(hidden_size, n_classes, bias=True)\n","\n","    def forward(self, x):\n","\n","        x = self.dropout(x)\n","        x = F.relu(self.hidden1(x))\n","        x = self.hidden1_dropout(x)\n","        x = F.relu(self.hidden2(x))\n","        x = self.hidden2_dropout(x)\n","        x = self.hidden3(x)\n","        return x\n","\n","\n","\n","class MnistDataset(Dataset):\n","    def __init__(self, data, target, transformation=None):\n","        self.images = data\n","        self.targets = target\n","        self.transformation = transforms.Compose([\n","              transforms.RandomAffine(0, (1/14, 1/14)),\n","              transforms.Normalize((0.5,), (0.5,))\n","            ])\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","\n","    def __getitem__(self, idx):\n","        return self.images[idx], self.targets[idx]\n","\n","\n","class MnistQs():\n","      def __init__(self):\n","          self.df = pd.read_pickle(\"u_methods.csv\")\n","          self.u_CE = self.df[\"u_CE\"]\n","          self.u_MFPS = self.df[\"u_MFPS\"]\n","          self.u_MFLS = self.df[\"u_MFLS\"]\n","\n","      def __len__(self):\n","          return self.df.shape[0]\n","\n","      def __getitem__(self, idx):\n","          return self.u_CE.iloc[idx], self.u_MFPS.iloc[idx], self.u_MFLS.iloc[idx]\n","\n","      def get_u_CE(self):\n","          self.u_CE = torch.zeros(30000, 10)\n","\n","          for idx, u in enumerate(self.df.u_CE):\n","              self.u_CE[idx, :] = torch.tensor(u)\n","\n","          return self.u_CE\n","\n","      def get_u_MFPS(self):\n","          self.u_MFPS = torch.zeros(30000, 10)\n","\n","          for idx, u in enumerate(self.df.u_MFPS):\n","              self.u_MFPS[idx, :] = torch.tensor(u)\n","          \n","          return self.u_MFPS\n","\n","      def get_u_MFLS(self):\n","          self.u_MFLS = torch.zeros(30000, 10)\n","\n","          for idx, u in enumerate(self.df.u_MFLS):\n","              self.u_MFLS[idx, :] = torch.tensor(u)\n","          \n","          return self.u_MFLS\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNMq0X6dlOM8"},"source":["# LOAD THE DATASET AND THE TEACHER FILES"]},{"cell_type":"code","metadata":{"id":"sCChJiNC5N-I"},"source":["images, targets = torch.load(\"/content/data/MNIST/processed/test.pt\")\n","images_train, targets_train = torch.load(\"/content/data/MNIST/processed/training.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmPFPSOF5XHh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620121983709,"user_tz":-120,"elapsed":11405,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"dbb7affb-38e2-462e-8912-745b2377b44c"},"source":["teacher1 = torch.load(\"teacher0to6.pt\")\n","teacher1.to(\"cuda\")\n","teacher2 = torch.load(\"teacher3to9.pt\")\n","teacher2.to(\"cuda\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (hidden1): Linear(in_features=784, out_features=1200, bias=True)\n","  (hidden1_dropout): Dropout(p=0.5, inplace=False)\n","  (hidden2): Linear(in_features=1200, out_features=1200, bias=True)\n","  (hidden2_dropout): Dropout(p=0.5, inplace=False)\n","  (hidden3): Linear(in_features=1200, out_features=7, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"_BXoAglnlnUM"},"source":["# FIRST METHOD (CROSS ENTROPY)"]},{"cell_type":"markdown","metadata":{"id":"S-l8W9uCouz5"},"source":["## Implement the gradient for the cross-entropy"]},{"cell_type":"code","metadata":{"id":"DOx0tuoIowlR"},"source":["def grad_j(dict_probs_t1, dict_probs_t2, u):\n","    grad_j = np.random.rand(10)\n","    for i,u_i in enumerate(u):\n","        dui = 0\n","        if i in dict_probs_t1.keys():\n","            dui = dui - dict_probs_t1[i]\n","            e = np.exp(u_i)/np.sum(np.exp(list(dict_probs_t1.values())))\n","            dui = dui + np.sum(np.array(list(dict_probs_t1.values()))*e)\n","        if i in dict_probs_t2.keys():\n","            dui = dui - dict_probs_t2[i]\n","            e = np.exp(u_i)/np.sum(np.exp(list(dict_probs_t2.values())))\n","            dui = dui + np.sum(np.array(list(dict_probs_t2.values()))*e)\n","        grad_j[i] = dui\n","    return grad_j\n","\n","\n","def ce_method1(image):\n","    iters = 3000\n","    m = nn.Softmax(dim=1)\n","    m2 = nn.Softmax(dim=0)\n","    # image = image.to(\"cuda\")\n","\n","    # Obtain logits from teacher\n","    z1 = teacher1(image.reshape(1, 784).float())\n","    z2 = teacher2(image.reshape(1, 784).float())\n","\n","    probs_t1 = m(z1).cpu().data.numpy()[0] \n","    probs_t2 = m(z2).cpu().data.numpy()[0]\n","\n","    dict_probs_t1 = {idx:probs_t1[idx] for idx in range(7)}\n","    dict_probs_t2 = {idx:probs_t2[idx-3] for idx in range(3, 10)}\n","\n","    u = np.random.rand(10)\n","    for it in range(iters):\n","        u = u - 0.1 * grad_j(dict_probs_t1, dict_probs_t2, u)\n","\n","\n","    q2 = m2(torch.from_numpy(u))\n","\n","    return dict_probs_t1, dict_probs_t2, u, q2\n","\n","\n","def ce_method1_batch(image_batch):\n","    iters = 3000\n","    m = nn.Softmax(dim=1)\n","    m2 = nn.Softmax(dim=0)\n","    # u_batch = torch.zeros(image_batch.shape[0], 10).to(\"cuda\")\n","    u_batch = torch.zeros(image_batch.shape[0], 10)\n","\n","    for idx, image in enumerate(image_batch):\n","        print(idx)\n","        # Obtain logits from teacher\n","        z1 = teacher1(image.float())\n","        z2 = teacher2(image.float())\n","\n","        probs_t1 = m2(z1).cpu().data.numpy() \n","        probs_t2 = m2(z2).cpu().data.numpy()\n","\n","        dict_probs_t1 = {idx:probs_t1[idx] for idx in range(7)}\n","        dict_probs_t2 = {idx:probs_t2[idx-3] for idx in range(3, 10)}\n","\n","        u = np.random.rand(10)\n","        for it in range(iters):\n","            u = u - 0.1 * grad_j(dict_probs_t1, dict_probs_t2, u)\n","        \n","        u_batch[idx, :] = torch.tensor(u)\n","\n","    return u_batch  # batch of logits\n","\n","\n","\n","def ce_method1_csv(image, p1, p2):\n","    iters = 3000\n","\n","    dict_probs_t1 = {idx:p1[idx] for idx in range(7)}\n","    dict_probs_t2 = {idx:p2[idx-3] for idx in range(3, 10)}\n","\n","    u = np.random.rand(10)\n","    for it in range(iters):\n","        u = u - 0.1 * grad_j(dict_probs_t1, dict_probs_t2, u)\n","\n","    return u"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Veyl-ielnc8k"},"source":["## Load test images and get logits and probs"]},{"cell_type":"code","metadata":{"id":"QcFbwGYInZEk"},"source":["img1 = images[2].to(\"cuda\") #1\n","img2 = images[4].to(\"cuda\") #4\n","img3 = images[0].to(\"cuda\") #7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXQLy6-Vn4iN"},"source":["z1_1 = teacher1(img1.reshape(1, 784).float()) #logits teacher1 per num 1\n","z1_2 = teacher2(img1.reshape(1, 784).float()) #logits teacher2 per num 1\n","\n","z2_1 = teacher1(img2.reshape(1, 784).float()) #logits teacher1 per num 4\n","z2_2 = teacher2(img2.reshape(1, 784).float()) #logits teacher2 per num 4\n","\n","z3_1 = teacher1(img3.reshape(1, 784).float()) #logits teacher1 per num 7\n","z3_2 = teacher2(img3.reshape(1, 784).float()) #logits teacher2 per num 7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iamyhKv9oYeb","executionInfo":{"status":"ok","timestamp":1619347428946,"user_tz":-120,"elapsed":786,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"ed555abf-ca42-4732-de9f-469ef668875b"},"source":["m = nn.Softmax(dim=1)\n","\n","probs_t1 = m(z3_1)\n","probs_t2 = m(z3_2)\n","\n","probs_t1 = probs_t1.cpu().data.numpy()\n","probs_t2 = probs_t2.cpu().data.numpy()\n","\n","dict_probs_t1 = {}\n","dict_probs_t2 = {}\n","\n","for idx in range(7):\n","    dict_probs_t1[idx] = probs_t1[0][idx]\n","\n","for idx in range(3, 10):\n","    dict_probs_t2[idx] = probs_t2[0][idx - 3]\n","\n","print(dict_probs_t1)\n","print(dict_probs_t2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{0: 1.3191016e-15, 1: 1.5482159e-15, 2: 1.6019008e-16, 3: 3.744862e-08, 4: 2.728826e-16, 5: 1.0, 6: 4.1164277e-11}\n","{3: 6.393586e-05, 4: 9.513434e-16, 5: 0.999936, 6: 8.791197e-11, 7: 2.1728551e-11, 8: 6.504364e-08, 9: 7.016933e-09}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BNiOHLQgqG1z"},"source":["## Get the q for each train image (sampling 10 for testing purpose)"]},{"cell_type":"code","metadata":{"id":"BbmgLE68oh6W","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1619353266714,"user_tz":-120,"elapsed":616,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"a5559d00-5751-4824-ddfc-6c0cd4d40195"},"source":["iters = 3000\n","m = nn.Softmax(dim=1)\n","m2 = nn.Softmax()\n","q2 = torch.rand([images_train.shape[0],10])\n","\n","for i in range(images_train.shape[0])[:10]:\n","    # Obtain image\n","    img = images_train[i].to(\"cuda\")\n","    \n","    # Obtain logits from teacher\n","    z1 = teacher1(img.reshape(1, 784).float())\n","    z2 = teacher2(img.reshape(1, 784).float())\n","    \n","    # compute softmax to obtain p_i\n","    probs_t1 = m(z1).cpu().data.numpy()[0] \n","    probs_t2 = m(z2).cpu().data.numpy()[0]\n","    print(probs_t1)\n","    print(probs_t2)\n","\n","    dict_probs_t1 = {idx:probs_t1[idx] for idx in range(7)}\n","    dict_probs_t2 = {idx:probs_t2[idx-3] for idx in range(3, 10)}\n","\n","    # compute gradient descent\n","    u = np.random.rand(10)\n","    for it in range(iters):\n","        u = u - 0.1 * grad_j(dict_probs_t1, dict_probs_t2, u)\n","    \n","    # compute softmax to obtain q\n","    q2[i] = m2(torch.from_numpy(u))\n","    if i%100==0: print(f'Computing q of image: {i}')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-238dba6a6433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Obtain logits from teacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'float32'"]}]},{"cell_type":"markdown","metadata":{"id":"dMlPs3QJrIiG"},"source":["# SECOND METHOD (MATRIX FACTORIZATION)"]},{"cell_type":"markdown","metadata":{"id":"ZZu46btHsIvM"},"source":["## INITIALIZE M, P, Z"]},{"cell_type":"code","metadata":{"id":"lb8S4Z635gIK"},"source":["M = np.array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","              [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n","M = M.T\n","\n","def get_PZ_matrices(image, M):\n","    # image = image.to(\"cuda\")\n","    z1 = teacher1(image.reshape(1, 784).float())\n","    z2 = teacher2(image.reshape(1, 784).float())\n","\n","    Z = np.zeros(M.shape)\n","    Z[:7, 0] = z1.cpu().data.numpy()\n","    Z[3:, 1] = z2.cpu().data.numpy()\n","\n","    m = nn.Softmax(dim=1)\n","    prob1 = m(z1)\n","    prob2 = m(z2)\n","\n","    P = np.zeros(M.shape)\n","    P[:7, 0] = prob1.cpu().data.numpy()\n","    P[3:, 1] = prob2.cpu().data.numpy()\n","\n","    return P, Z\n","\n","def get_PZ_matrices_batch(image, M):\n","    # image = image.to(\"cuda\")\n","    z1 = teacher1(image.float())\n","    z2 = teacher2(image.float())\n","\n","    Z = np.zeros(M.shape)\n","    Z[:7, 0] = z1.cpu().data.numpy()\n","    Z[3:, 1] = z2.cpu().data.numpy()\n","\n","    m = nn.Softmax(dim=0)\n","    prob1 = m(z1)\n","    prob2 = m(z2)\n","\n","    P = np.zeros(M.shape)\n","    P[:7, 0] = prob1.cpu().data.numpy()\n","    P[3:, 1] = prob2.cpu().data.numpy()\n","\n","    return P, Z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qG2N7JjIsVUh"},"source":["## CODE FOR MF IN PROBABILITY AND LOGIT SPACE"]},{"cell_type":"code","metadata":{"id":"jNCl31LXi9Ao"},"source":["import numpy as np\n","from sklearn.metrics import mean_squared_error as RMSE\n","\n","\n","# MATRIX FACTORIZATION IN PROBABILITY SPACE\n","def mf_prob_space(M, P):\n","    # Parameter initialization\n","    L, N = M.shape\n","    v = np.ones(N)\n","    u = np.ones(L)\n","\n","    u_k = u.copy() * 2\n","    v_k = v.copy() * 2\n","    iters = 0\n","\n","    # Run until convergence\n","    while RMSE(u, u_k) > 1e-3 and RMSE(v, v_k) > 1e-3 or iters < 3000:\n","        u_k = u.copy()\n","        for j in range(L):\n","            # First for loop\n","            u[j] = np.sum(M[j, :] * P[j, :] * v) / np.sum(M[j, :] * np.power(v, 2))\n","            u[j] = max(0, u[j])\n","\n","            u = u / np.sum(u)\n","\n","        for i in range(N):\n","            v[i] = np.sum(M[:, i] * P[:, i] * u) / np.sum(M[:, i] * np.power(u, 2))\n","            v[i] = max(0, v[i])\n","        \n","        iters += 1\n","\n","    \n","    # print(f\"u converged after {iters} iterations.\")\n","    return u, v\n","\n","\n","def mf_prob_space_batch(image_batch, M):\n","    # Parameter initialization\n","    L, N = M.shape\n","    u_batch = np.zeros((image_batch.shape[0], 10))\n","\n","    for idx, image in enumerate(image_batch):\n","        P, Z = get_PZ_matrices_batch(image, M)\n","        v = np.ones(N)\n","        u = np.ones(L)\n","\n","        u_k = u.copy() * 2\n","        v_k = v.copy() * 2\n","        iters = 0\n","\n","        # Run until convergence\n","        while RMSE(u, u_k) > 1e-3 and RMSE(v, v_k) > 1e-3 or iters < 3000:\n","            u_k = u.copy()\n","            for j in range(L):\n","                # First for loop\n","                u[j] = np.sum(M[j, :] * P[j, :] * v) / np.sum(M[j, :] * np.power(v, 2))\n","                u[j] = max(0, u[j])\n","\n","                u = u / np.sum(u)\n","\n","            for i in range(N):\n","                v[i] = np.sum(M[:, i] * P[:, i] * u) / np.sum(M[:, i] * np.power(u, 2))\n","                v[i] = max(0, v[i])\n","            \n","            iters += 1\n","\n","        u_batch[idx, :] = u\n","    \n","    print(\"Batch done!\")\n","    return u_batch\n","\n","\n","# MATRIX FACTORIZATION IN LOGIT SPACE\n","def mf_logit_space(M, Z):\n","    lambd = 0.01\n","    L, N = M.shape\n","\n","    v = np.ones(N)\n","    u = np.ones(L)\n","    c = np.ones(N)\n","\n","    u_k = u.copy() * 2\n","    v_k = v.copy() * 2\n","    iters = 0\n","\n","    # Initialize c\n","    for i in range(N):\n","      c[i] = np.sum(M[:, i] * Z[:, i]) / np.sum(M[:, i])\n","\n","\n","    # Run until convergence\n","    while RMSE(u, u_k) > 1e-3 and RMSE(v, v_k) > 1e-3 or iters < 3000:\n","        u_k = u.copy()\n","        v_k = v.copy()\n","\n","        for j in range(L):\n","            # First for loop\n","            u[j] = np.sum(M[j, :] * (Z[j, :] - c) * v) / (lambd + np.sum(M[j, :] * np.power(v, 2)))\n","\n","        for i in range(N):\n","            v[i] = np.sum(M[:, i] * (Z[:, i]) * u) / (lambd + np.sum(M[:, i] * np.power(u, 2)))\n","            v[i] = max(0, v[i])\n","\n","            c[i] = np.sum(M[:, i] * (Z[:, i] - u * v[i])) / np.sum(M[:, i])\n","      \n","\n","        iters += 1\n","    # print(f\"u converged after {iters} iterations.\")\n","\n","    return u, v, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Glb-ijXm48sn"},"source":["def get_PZ_matrices_csv(image, M, z1, z2, p1, p2):\n","    Z = np.zeros(M.shape)\n","    Z[:7, 0] = z1.cpu().data.numpy()\n","    Z[3:, 1] = z2.cpu().data.numpy()\n","\n","    P = np.zeros(M.shape)\n","    P[:7, 0] = p1\n","    P[3:, 1] = p2\n","\n","    return P, Z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRitRT2sBYLF"},"source":["# Save soft labels for each image in a csv file"]},{"cell_type":"code","metadata":{"id":"safmJEhJQKtd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af15c00d-1224-4f13-c2a2-97c5b12fa4a2"},"source":["m = nn.Softmax(dim=1)\n","M = np.array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","              [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n","M = M.T\n","\n","u_df = pd.DataFrame()\n","\n","for i, img in enumerate(images_train[:16384]):\n","    # img = img.to(\"cuda\")\n","    z1 = teacher1(img.reshape(1, 784).float())\n","    z2 = teacher2(img.reshape(1, 784).float())\n","\n","    probs_t1 = m(z1).cpu().data.numpy()[0] \n","    probs_t2 = m(z2).cpu().data.numpy()[0]\n","\n","    P, Z = get_PZ_matrices_csv(img, M, z1, z2, probs_t1, probs_t2)\n","\n","    # Get u from Cross-Entropy method 1\n","    u_CE = ce_method1_csv(img, probs_t1, probs_t2)\n","\n","    # Get u from MF probability space method 2\n","    u_MFPS, _ = mf_prob_space(M, P)\n","\n","    # Get u from MF logit space method 2\n","    u_MFLS, _, _ = mf_logit_space(M, Z)\n","\n","    u_df = u_df.append({\n","        \"u_CE\": np.array(u_CE),\n","        \"u_MFPS\": np.array(u_MFPS),\n","        \"u_MFLS\": np.array(u_MFLS)\n","    }, ignore_index=True)\n","\n","    if i%100==0: print(i)\n","\n","# u_df.to_csv(\"u_methods.csv\", index=False)\n","u_df.to_pickle(\"u_methods.csv\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x2sRo5zDsokQ"},"source":["## Experiments"]},{"cell_type":"code","metadata":{"id":"U9q1vrIesm3-"},"source":["img1 = images[2].to(\"cuda\") #1\n","img2 = images[4].to(\"cuda\") #4\n","img3 = images[0].to(\"cuda\") #7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7--KzOQJszDc","executionInfo":{"status":"ok","timestamp":1619358317660,"user_tz":-120,"elapsed":1802,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"b68b8be7-3dda-42ed-8a87-a0df69075420"},"source":["p1, p2, q = ce_method1(img3)\n","p1, p2, q"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({0: 0.016459377,\n","  1: 0.00024381149,\n","  2: 0.8683283,\n","  3: 0.10226064,\n","  4: 0.0058595897,\n","  5: 0.00679617,\n","  6: 5.2110398e-05},\n"," {3: 1.0885118e-06,\n","  4: 6.21561e-08,\n","  5: 7.412574e-10,\n","  6: 6.7759192e-12,\n","  7: 0.9999881,\n","  8: 8.296881e-09,\n","  9: 1.0719809e-05},\n"," tensor([0.0084, 0.0017, 0.4387, 0.0261, 0.0018, 0.0020, 0.0009, 0.5170, 0.0017,\n","         0.0017], dtype=torch.float64))"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIZ_lC6PxWAm","executionInfo":{"status":"ok","timestamp":1618860754278,"user_tz":-120,"elapsed":9267,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"5cb5803f-b9de-460f-b004-e0a30ae72b72"},"source":["P, Z = get_PZ_matrices(img3, M)\n","q, v = mf_prob_space(M, P)\n","q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["u converged after 3000 iterations.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([8.96863583e-16, 3.00439310e-15, 2.66815065e-14, 3.73005951e-04,\n","       3.96062008e-14, 9.99626953e-01, 4.47262278e-11, 2.10652293e-13,\n","       1.34281626e-09, 4.01277446e-08])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wnofunw-0syg","executionInfo":{"status":"ok","timestamp":1618860755910,"user_tz":-120,"elapsed":10890,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"cf54dc8d-95f2-4cfa-f5b9-122cb82fa81c"},"source":["q, v, c = mf_logit_space(M, Z)\n","q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["u converged after 3000 iterations.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["tensor([1.8968e-31, 2.2811e-30, 2.0386e-28, 1.0673e-13, 1.4766e-31, 1.0000e+00,\n","        9.5436e-24, 1.2873e-31, 8.0476e-23, 2.0744e-19], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"-Av2AqFeYyzG"},"source":["# IMPLEMENTING THE STUDENT"]},{"cell_type":"code","metadata":{"id":"sDBt_SqRbtg4"},"source":["images, targets = torch.load(\"/content/data/MNIST/processed/training.pt\")\n","images = images[:30000]\n","targets = targets[:30000]\n","trainset = MnistDataset(images, targets)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=125, shuffle=False, num_workers=2)\n","mnistq = MnistQs()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11_YXrZiubul"},"source":["student_CE = Model(n_classes=10, hidden_size=500, dropout=0.1, hidden_dropout=0.1)\n","student_CE.to(\"cuda\")\n","\n","criterion = nn.KLDivLoss()\n","m = nn.Softmax(dim=0)\n","learning_rate = 0.001\n","epochs = 50\n","\n","# Set up loss function and optimizer\n","optimizer = optim.SGD(student_CE.parameters(), lr=learning_rate, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-gy17UcAKiN"},"source":["# Training student with method 1"]},{"cell_type":"code","metadata":{"id":"yIpajMSFYxKD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620125131742,"user_tz":-120,"elapsed":59020,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"7a4ee8ab-9380-4b8d-eac0-04834182b083"},"source":["# Run over 1000 epochs (1 epoch = visited all items in dataset)\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    total = 0\n","\n","    for image, qs in zip(trainloader, np.array_split(mnistq.get_u_CE(), 240)):\n","        # Apply the learning rate decay\n","        if(epoch % 100 == 0 and epoch != 0):\n","            learning_rate = learning_rate * 0.5\n","            optimizer = optim.SGD(student_CE.parameters(), lr= learning_rate, momentum=0.9)\n","        \n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, targets = image\n","        inputs = torch.flatten(inputs, start_dim=1).to(\"cuda\")\n","        qs = qs.to(\"cuda\")\n","        # target = labels.to(\"cuda\").long()\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Set temperature and the weights for losses linear combination\n","        w = 0.7\n","        T = 3\n","\n","        # Student forward + backward + optimize\n","        logits_student = student_CE(inputs.float())\n","        # loss = torch.cdist(m(qs), m(logits_student), p=2)\n","        loss = criterion(F.log_softmax(qs/T, dim=1), F.softmax(logits_student/T, dim=1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total += len(image)\n","\n","        # print statistics\n","        running_loss += loss.item()\n","    # print every epoch\n","    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2611: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["[1] loss: 0.038\n","[2] loss: 0.020\n","[3] loss: 0.015\n","[4] loss: 0.012\n","[5] loss: 0.011\n","[6] loss: 0.010\n","[7] loss: 0.009\n","[8] loss: 0.009\n","[9] loss: 0.008\n","[10] loss: 0.008\n","[11] loss: 0.008\n","[12] loss: 0.008\n","[13] loss: 0.007\n","[14] loss: 0.007\n","[15] loss: 0.007\n","[16] loss: 0.007\n","[17] loss: 0.007\n","[18] loss: 0.007\n","[19] loss: 0.007\n","[20] loss: 0.007\n","[21] loss: 0.006\n","[22] loss: 0.006\n","[23] loss: 0.006\n","[24] loss: 0.006\n","[25] loss: 0.006\n","[26] loss: 0.006\n","[27] loss: 0.006\n","[28] loss: 0.006\n","[29] loss: 0.006\n","[30] loss: 0.006\n","[31] loss: 0.006\n","[32] loss: 0.006\n","[33] loss: 0.006\n","[34] loss: 0.006\n","[35] loss: 0.006\n","[36] loss: 0.006\n","[37] loss: 0.006\n","[38] loss: 0.005\n","[39] loss: 0.005\n","[40] loss: 0.005\n","[41] loss: 0.005\n","[42] loss: 0.005\n","[43] loss: 0.005\n","[44] loss: 0.005\n","[45] loss: 0.005\n","[46] loss: 0.005\n","[47] loss: 0.005\n","[48] loss: 0.005\n","[49] loss: 0.005\n","[50] loss: 0.005\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vZ8QrWTAepS6"},"source":["# Testing student with method 1"]},{"cell_type":"code","metadata":{"id":"GmlcMHLDd7C4"},"source":["images_test, targets_test = torch.load(\"/content/data/MNIST/processed/test.pt\")\n","testset = MnistDataset(images_test, targets_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RydXmyvIcDJK","executionInfo":{"status":"ok","timestamp":1620125184592,"user_tz":-120,"elapsed":934,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"34e11ead-18f3-4f1e-d742-93d532d681ee"},"source":["# Define support function used to convert label to one-hot encoded tensor\n","def convert_labels(labels):\n","    target = torch.zeros([len(labels), 10], dtype=torch.float32)\n","    for i, l in enumerate(labels):\n","      target[i][l] = 1.0\n","    return target\n","\n","# Run model on test set and determine accuracy\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs = torch.flatten(inputs, start_dim=1).to(\"cuda\")\n","        target = convert_labels(labels).to(\"cuda\")\n","        outputs = student_CE(inputs.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","        _, target = torch.max(target.data, 1)\n","        total += target.size(0)\n","        correct += (predicted == target).sum().item()\n","        # for i, val in enumerate(predicted):\n","        #   wrong[target[i]][val] += 1\n","\n","# Output model accuracy to user\n","print('Accuracy of the network on test images: %d %% (%d wrong out of %d)' % (\n","    100 * correct / total, total - correct, total))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on test images: 82 % (1755 wrong out of 10000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u8ZTn_hKAQCS"},"source":["# Training student with method 2 (MFPS)"]},{"cell_type":"code","metadata":{"id":"QCNZY5HeAmlo"},"source":["student_MFPS = Model(n_classes=10, hidden_size=500, dropout=0.1, hidden_dropout=0.1)\n","student_MFPS.to(\"cuda\")\n","\n","# bce_with_logits = torch.nn.BCEWithLogitsLoss()\n","learning_rate = 0.001\n","epochs = 50\n","\n","# Set up loss function and optimizer\n","optimizer = optim.SGD(student_MFPS.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y82aZoYipsyF","colab":{"base_uri":"https://localhost:8080/","height":438},"executionInfo":{"status":"error","timestamp":1620126389044,"user_tz":-120,"elapsed":12632,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"dd48d465-eb76-4c88-bd85-c69c525fdd67"},"source":["for epoch in range(epochs):\n","    running_loss = 0.0\n","    total = 0\n","\n","    for image, qs in zip(trainloader, np.array_split(mnistq.get_u_MFPS(), 240)):\n","        # Apply the learning rate decay\n","        if(epoch % 100 == 0 and epoch != 0):\n","            learning_rate = learning_rate * 0.5\n","            optimizer = optim.SGD(student_MFPS.parameters(), lr= learning_rate, momentum=0.9)\n","        \n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, targets = image\n","        inputs = torch.flatten(inputs, start_dim=1).to(\"cuda\")\n","        \n","        qs = qs.to(\"cuda\")\n","        # target = labels.to(\"cuda\").long()\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Set temperature and the weights for losses linear combination\n","        w = 0.7\n","        T = 3\n","\n","        # Student forward + backward + optimize\n","        logits_student = student_MFPS(inputs.float())\n","        qs = torch.log(qs/(1 - qs))\n","        loss = criterion(F.log_softmax(qs/T, dim=1), F.softmax(logits_student/T, dim=1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total += len(image)\n","\n","        # print statistics\n","        running_loss += loss.item()\n","    # print every epoch\n","    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2611: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["[1] loss: nan\n","[2] loss: 0.000\n","[3] loss: 0.000\n","[4] loss: 0.000\n","[5] loss: 0.000\n","[6] loss: 0.000\n","[7] loss: 0.000\n","[8] loss: 0.000\n","[9] loss: 0.000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-114-9e53c978c598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnistq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_u_MFPS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Apply the learning rate decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"xokyrdEfAVfZ"},"source":["# Training student with method 2 (MFLS)"]},{"cell_type":"code","metadata":{"id":"yuQUz13nBIP1"},"source":["student_MFLS = Model(n_classes=10, hidden_size=500, dropout=0.1, hidden_dropout=0.1)\n","student_MFLS.to(\"cuda\")\n","\n","criterion = nn.KLDivLoss()\n","m = nn.Softmax(dim=0)\n","learning_rate = 0.001\n","epochs = 50\n","\n","# Set up loss function and optimizer\n","optimizer = optim.SGD(student_MFLS.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcbnEYKnOcg0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620125681541,"user_tz":-120,"elapsed":60467,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"dae8c0af-3cbf-43fd-8b8f-cdbccf4a3cb3"},"source":["for epoch in range(epochs):\n","    running_loss = 0.0\n","    total = 0\n","\n","    for image, qs in zip(trainloader, np.array_split(mnistq.get_u_MFLS(), 240)):\n","        # Apply the learning rate decay\n","        if(epoch % 100 == 0 and epoch != 0):\n","            learning_rate = learning_rate * 0.5\n","            optimizer = optim.SGD(student_MFLS.parameters(), lr= learning_rate, momentum=0.9)\n","        \n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, targets = image\n","        inputs = torch.flatten(inputs, start_dim=1).to(\"cuda\")\n","        qs = qs.to(\"cuda\")\n","        # target = labels.to(\"cuda\").long()\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Set temperature and the weights for losses linear combination\n","        w = 0.7\n","        T = 3\n","\n","        # Student forward + backward + optimize\n","        logits_student = student_MFLS(inputs.float())\n","        loss = criterion(F.log_softmax(qs/T, dim=1), F.softmax(logits_student/T, dim=1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total += len(image)\n","\n","        # print statistics\n","        running_loss += loss.item()\n","    # print every epoch\n","    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2611: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["[1] loss: 0.188\n","[2] loss: 0.048\n","[3] loss: 0.037\n","[4] loss: 0.032\n","[5] loss: 0.030\n","[6] loss: 0.028\n","[7] loss: 0.026\n","[8] loss: 0.025\n","[9] loss: 0.024\n","[10] loss: 0.023\n","[11] loss: 0.022\n","[12] loss: 0.022\n","[13] loss: 0.021\n","[14] loss: 0.021\n","[15] loss: 0.021\n","[16] loss: 0.020\n","[17] loss: 0.020\n","[18] loss: 0.020\n","[19] loss: 0.019\n","[20] loss: 0.019\n","[21] loss: 0.019\n","[22] loss: 0.018\n","[23] loss: 0.018\n","[24] loss: 0.018\n","[25] loss: 0.018\n","[26] loss: 0.018\n","[27] loss: 0.017\n","[28] loss: 0.017\n","[29] loss: 0.017\n","[30] loss: 0.017\n","[31] loss: 0.017\n","[32] loss: 0.017\n","[33] loss: 0.017\n","[34] loss: 0.017\n","[35] loss: 0.017\n","[36] loss: 0.016\n","[37] loss: 0.016\n","[38] loss: 0.017\n","[39] loss: 0.016\n","[40] loss: 0.016\n","[41] loss: 0.016\n","[42] loss: 0.016\n","[43] loss: 0.016\n","[44] loss: 0.016\n","[45] loss: 0.016\n","[46] loss: 0.016\n","[47] loss: 0.016\n","[48] loss: 0.016\n","[49] loss: 0.016\n","[50] loss: 0.015\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hXu1ZRhYBTs7"},"source":["# Testing student with method 2 MFLS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mE-ClvWVBCKB","executionInfo":{"status":"ok","timestamp":1620125736609,"user_tz":-120,"elapsed":1321,"user":{"displayName":"Aitor Lucas","photoUrl":"","userId":"07298394126490224130"}},"outputId":"eddf562a-478e-4b51-ef6a-ce0fc17e1698"},"source":["# Define support function used to convert label to one-hot encoded tensor\n","def convert_labels(labels):\n","    target = torch.zeros([len(labels), 10], dtype=torch.float32)\n","    for i, l in enumerate(labels):\n","      target[i][l] = 1.0\n","    return target\n","\n","# Run model on test set and determine accuracy\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs = torch.flatten(inputs, start_dim=1).to(\"cuda\")\n","        target = convert_labels(labels).to(\"cuda\")\n","        outputs = student_MFLS(inputs.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","        _, target = torch.max(target.data, 1)\n","        total += target.size(0)\n","        correct += (predicted == target).sum().item()\n","        # for i, val in enumerate(predicted):\n","        #   wrong[target[i]][val] += 1\n","\n","# Output model accuracy to user\n","print('Accuracy of the network on test images: %d %% (%d wrong out of %d)' % (\n","    100 * correct / total, total - correct, total))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on test images: 95 % (433 wrong out of 10000)\n"],"name":"stdout"}]}]}